\documentclass[en]{pracamgr}

\autor{Grzegorz Kwacz}{429554}
\title{Clustering voters by their preferences}
\titlepl{Klastrowanie wyborców w oparciu o ich preferencje}
\kierunek{Computer Science}
\opiekun{dr hab. Piotr Skowron\\
  Instytut Informatyki\\
  }

\date{June 2025}

\dziedzina{
%11.0 Matematyka, Informatyka:\\
%11.1 Matematyka\\
%11.2 Statystyka\\
11.3 Informatyka\\
%11.4 Sztuczna inteligencja\\
%11.5 Nauki aktuarialne\\
%11.9 Inne nauki matematyczne i informatyczne
}

%Klasyfikacja tematyczna wedlug AMS (matematyka) lub ACM (informatyka)
\klasyfikacja{D. Software\\
  D.127. Blabalgorithms\\
  D.127.6. Numerical blabalysis}

% Słowa kluczowe:
\keywords{blabaliza różnicowa, fetory $\sigma$-$\rho$, fooizm,
  blarbarucja, blaba, fetoryka, baleronik}

% Tu jest dobre miejsce na Twoje własne makra i~środowiska:
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{indentfirst}
\newtheorem{defi}{Definicja}[section]

% koniec definicji

\begin{document}

\maketitle

\begin{abstract}
W pracy zastosowano algorytmy klastrowania do danych wyborczych w głosowaniu
przez aprobaty na budżet partycypacyjny, uwzględniając zarówno dane z faktycznych
wyborów, jak i modele syntetyczne. Przeanalizowano różne definicje odległości
głosów pod kątem efektywności znajdowania grup wyborców głosujących podobnie.
\end{abstract}

\tableofcontents
%\listoffigures
%\listoftables

\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

W ostatnim czasie w Teorii Wyboru Społecznego popularność zyskało wiele
aksjomatów proporcjonalności. Mówią one, że głos odpowiednio dużych grup
wyborców powinien być odpowiednio uwzględniony -- proporcjonalnie do wielkości
takiej grupy. Jednym z najbardziej popularnych aksjomatów jest EJR, który
dotyczy grup $\ell$-spójnych. O ile jest on często rozważany teoretycznie, w
praktyce okazuje się problematyczny. Pierwszą przeszkodą w użyciu go
w praktyce jest to, że zarówno znajdowanie grup $\ell$-spójnych, jak i
sama weryfikacja czy dany wynik spełnia ten aksjomat, są problemami
NP-trudnymi. Drugim niepożądanym zjawiskiem jest to, że w praktyce takie grupy
występują bardzo rzadko dla $\ell > 2$.

Aby rozwiązać te problemy, powstał aksjomat EJR+, który jest weryfikowalny w
czasie wielomianowym. ??

Podejście zastosowane w pracy ma wymiar praktyczny i używa uczenia maszynowego
do identyfikacji grup wyborców głosujących podobnie. W podziale X przetestowano
różne pojęcia podobieństwa głosów oraz różnice między nimi. Następnie w
rozdziale X użyto algorytmu $k$-median do klastrowania. Wszystkie eksperymenty
zostały przeprowadzone na danych z biblioteki Pabulib oraz na danych
wygenerowanych z modeli syntetycznych.

\chapter{Basic concepts}\label{r:pojecia}

\section{Approval voting}

\subsection{Definition}

We will denote the set of voters as $V = \{ v_i \ |\ i \in [n]\}$ and the set of
candidates (or projects) as $C = \{c_i \ |\ i \in [m] \}$. In Approval Voting,
each voter casts a ballot $A \subseteq C$, indicating candidates they
approve. We will denote the ballot of $v_i$ as $A_i$. The set of all possible
ballots is $\mathcal A = P(C)$.

Sometimes, we will interpret the ballots as $m$-dimensional vectors, where
$\vec A = \sum\limits_{c_i \in A} \vec e_i$.

\subsection{Synthetic models}

\subsubsection{Impartial culture}

The \textit{Impartial Culture} model is characterized by a single number
$p \in [0,1]$, which is a probability of a voter approving a candidate. A ballot
is generated by independently approving each candidate with this probability.

\subsubsection{Independent Approval}

The \textit{Independent Approval} model is characterized by a vector
$(p_1, p_2, \ldots, p_m) \in [0,1]^m$, where $p_i$ is the probability of
approving candidate $c_i$. A ballot is generated by independently approving the
candidates with respective probabilities.

\section{Similarity measures}

A \textit{similarity function} is any function
$s : \mathcal A \times \mathcal A \to [0,1]$.
A corresponding \textit{distance function} is a function
$d : \mathcal A \times \mathcal A \to [0,1]$, defined as $d(x,y) = 1 - s(x,y)$.

\subsection{Hamming distance}

A standard Hamming distance is defined as the number of candidates the voters
disagree on:
\[ d_H(A,B) = |A \Delta B| \]
To meet the normalization criterion, one could divide it by $m$.

\subsection{Jaccard similarity}

The \textit{Jaccard similarity} (also known as \textit{Intersection over Union})
is defined as
\[ s_J(A,B) = \frac{|A \cap B|}{|A \cup B|} \]

Thus, the \textit{Jaccard disntace} is
\[ d_J(A,B) = 1-\frac{|A \cap B|}{|A \cup B|} = \frac{d_H(A,B)}{|A \cup B|} \]
which can be seen as a different method of normalizing the
\textit{Hamming distance}.

\subsection{Euclidean distance}

A standard Euclidean distance is defined as
\[ d_E(\vec A, \vec B) = ||\vec A - \vec B|| = \sqrt{d_H(A,B)} \]
To meet the normalization criterion, one could divide it by $\sqrt m$.

\subsection{Cosine similarity}

The \textit{Cosine similarity} is defined as the cosine on the angle between
ballot vectors, that is:
\[ s_C(\vec A, \vec B) = \frac{\vec A \cdot \vec B}{||\vec A||\cdot||\vec B||} \]

% \subsection{Chord distance}
%
% The \textit{Chord distance} is a variaton of Euclidean distance, where the
% vectors are first normalized.

\subsection{SimRank}

The \textit{SimRank} similarity captures an intuition, that voters are
\textit{similar} if they vote for \textit{similar} candidates, and candidates
are \textit{similar}, when \textit{similar} voters vote for them. It defines two
mutually recursive functions for voters and candidates similarities. For
identical objects, they functions are defined to be 1, and for different ones,
the following formula is used:

\begin{align*}
  s_R^V(v_i, v_j) &= \frac{D}{|A_i||A_j|} \sum_{c_i \in A_i} \sum_{c_j \in A_j} s_R^C(c_1, c_2) \\
  s_R^C(c_i, c_j) &= \frac{D}{|V_i||V_j|} \sum_{v_i \in V_i} \sum_{v_j \in V_j} s_R^V(v_1, v_2)
\end{align*}

Where $V_i$ is the set of voters that approve candidate $c_i$, and $D \in (0,1)$
is a decay constant that guarantees the converrgance of the iterative method.
Note that different objects can have a maximum similarity of $D$.

% \subsection{RoleSim}

\section{Clustering}

\subsection{Definition and basic algorithms}

Clustering is a process that partitions a set into \textit{clusters} in a way
that elements in the same cluster are similar and the elements in different
clusters are not.

The most popular clustering algorithm is $k$-means, which works for data in
$\mathbb R^d$. It is a variant of \textit{Centroid clustering}, where each
cluster has a \textit{center} $\mu$. The number $k$ defines the number of
clusters. The goal of $k$-means is to find a clustering $S$ that minimizes
\[ \sum_{i=1}^k \sum_{\vec x \in S_i} ||\vec x - \mu_i||^2 \]
where $\mu_i$ is the arthmetic mean of the cluster $S_i$:
\[ \mu_i = \frac{1}{|S_i|} \sum\limits_{\vec x \in S_i}\vec x \]

The $k$-means algorithm could be used directly for the Euclidean and Chord
distances, but for a non-euclidean space, one has to use a different clustering
algorithm. The most similar one is $k$-medoids, which is also a variant of
Centroid clustering, where the centers are actual data points (medoids of the
clusters):
\[ \mu_i = \arg\min_{y \in S_i} \sum_{x \in S_i}d(y,x) \]
The objective function of $k$-medoids is
\[ \sum_{i=1}^k \sum_{x \in S_i} d(x,\mu_i) \]
This allows the use of an arbitrary distance function for clustering. The
$k$-medoids clustering is also more robust to outliers than $k$-means that
squares the distances.

\subsection{Assessing cluster quality}

\subsubsection{Simplified Silhouette score}

The \textit{Medoid Silhouette} technique allows to score how each sample has
been clustered. For a data point $x \in S_i$, it compares how close it is to
its cluster center and the closest other center:
\begin{align*}
  s(x) &= 1 - \frac{d(x,\mu_i)}{\min\limits_{j \neq i}d(x,\mu_j)}
\end{align*}

\chapter{Analysis of similarity functions}

\section{Distributions}

\section{Correlations}

\chapter{Clustering}

\chapter{Podsumowanie}

\appendix

\chapter{Resulting data and graphs}

\chapter{Przykładowe dane wejściowe algorytmu}

\begin{center}
  \begin{tabular}{rrr}
    $\alpha$ & $\beta$ & $\gamma_7$ \\
    901384 & 13784 & 1341\\
    68746546 & 13498& 09165\\
    918324719& 1789 & 1310 \\
    9089 & 91032874& 1873 \\
    1 & 9187 & 19032874193 \\
    90143 & 01938 & 0193284 \\
    309132 & $-1349$ & $-149089088$ \\
    0202122 & 1234132 & 918324098 \\
    11234 & $-109234$ & 1934 \\
  \end{tabular}
\end{center}

\chapter{Przykładowe wyniki blabalizy
    (ze~współczynnikami~$\sigma$-$\rho$)}

\begin{center}
  \begin{tabular}{lrrrr}
    & Współczynniki \\
    & Głombaskiego & $\rho$ & $\sigma$ & $\sigma$-$\rho$\\
    $\gamma_{0}$ & 1,331 & 2,01 & 13,42 & 0,01 \\
    $\gamma_{1}$ & 1,331 & 113,01 & 13,42 & 0,01 \\
    $\gamma_{2}$ & 1,332 & 0,01 & 13,42 & 0,01 \\
    $\gamma_{3}$ & 1,331 & 51,01 & 13,42 & 0,01 \\
    $\gamma_{4}$ & 1,332 & 3165,01 & 13,42 & 0,01 \\
    $\gamma_{5}$ & 1,331 & 1,01 & 13,42 & 0,01 \\
    $\gamma_{6}$ & 1,330 & 0,01 & 13,42 & 0,01 \\
    $\gamma_{7}$ & 1,331 & 16435,01 & 13,42 & 0,01 \\
    $\gamma_{8}$ & 1,332 & 865336,01 & 13,42 & 0,01 \\
    $\gamma_{9}$ & 1,331 & 34,01 & 13,42 & 0,01 \\
    $\gamma_{10}$ & 1,332 & 7891432,01 & 13,42 & 0,01 \\
    $\gamma_{11}$ & 1,331 & 8913,01 & 13,42 & 0,01 \\
    $\gamma_{12}$ & 1,331 & 13,01 & 13,42 & 0,01 \\
    $\gamma_{13}$ & 1,334 & 789,01 & 13,42 & 0,01 \\
    $\gamma_{14}$ & 1,331 & 4897453,01 & 13,42 & 0,01 \\
    $\gamma_{15}$ & 1,329 & 783591,01 & 13,42 & 0,01 \\
  \end{tabular}
\end{center}

\begin{thebibliography}{99}
\addcontentsline{toc}{chapter}{Bibliografia}

\bibitem[Bea65]{beaman} Juliusz Beaman, \textit{Morbidity of the Jolly
    function}, Mathematica Absurdica, 117 (1965) 338--9.

\bibitem[Blar16]{eb1} Elizjusz Blarbarucki, \textit{O pewnych
    aspektach pewnych aspektów}, Astrolog Polski, Zeszyt 16, Warszawa
  1916.

\bibitem[Fif00]{ffgg} Filigran Fifak, Gizbert Gryzogrzechotalski,
  \textit{O blabalii fetorycznej}, Materiały Konferencji Euroblabal
  2000.

\bibitem[Fif01]{ff-sr} Filigran Fifak, \textit{O fetorach
    $\sigma$-$\rho$}, Acta Fetorica, 2001.

\bibitem[Głomb04]{grglo} Gryzybór Głombaski, \textit{Parazytonikacja
    blabiczna fetorów --- nowa teoria wszystkiego}, Warszawa 1904.

\bibitem[Hopp96]{hopp} Claude Hopper, \textit{On some $\Pi$-hedral
    surfaces in quasi-quasi space}, Omnius University Press, 1996.

\bibitem[Leuk00]{leuk} Lechoslav Leukocyt, \textit{Oval mappings ab ovo},
  Materiały Białostockiej Konferencji Hodowców Drobiu, 2000.

\bibitem[Rozk93]{JR} Josip A.~Rozkosza, \textit{O pewnych własnościach
    pewnych funkcji}, Północnopomorski Dziennik Matematyczny 63491
  (1993).

\bibitem[Spy59]{spyrpt} Mrowclaw Spyrpt, \textit{A matrix is a matrix
    is a matrix}, Mat. Zburp., 91 (1959) 28--35.

\bibitem[Sri64]{srinis} Rajagopalachari Sriniswamiramanathan,
  \textit{Some expansions on the Flausgloten Theorem on locally
    congested lutches}, J. Math.  Soc., North Bombay, 13 (1964) 72--6.

\bibitem[Whi25]{russell} Alfred N. Whitehead, Bertrand Russell,
  \textit{Principia Mathematica}, Cambridge University Press, 1925.

\bibitem[Zen69]{heu} Zenon Zenon, \textit{Użyteczne heurystyki
    w~blabalizie}, Młody Technik, nr~11, 1969.

\end{thebibliography}

\end{document}

